
diff --git a/reproduce.py b/reproduce.py
new file mode 100644
index 000000000..b91c81180
--- /dev/null
+++ b/reproduce.py
@@ -0,0 +1,10 @@
+import numpy as np
+from sklearn import linear_model as lm
+
+# test database
+n = 100
+x = np.random.randn(n, 30)
+y = np.random.normal(size=n)
+
+rr = lm.RidgeClassifierCV(alphas=np.arange(0.1, 1000, 0.1), normalize=True, 
+                          store_cv_values=True).fit(x, y)
diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py
index 911c74ae3..d322bcc30 100644
--- a/sklearn/linear_model/ridge.py
+++ b/sklearn/linear_model/ridge.py
@@ -4,97 +4,6 @@ Ridge regression
 
 # Author: Mathieu Blondel <mathieu@mblondel.org>
 #         Reuben Fletcher-Costin <reuben.fletchercostin@gmail.com>
-#         Fabian Pedregosa <fabian@fseoane.net>
-#         Michael Eickenberg <michael.eickenberg@nsup.org>
-# License: BSD 3 clause
-
-
-from abc import ABCMeta, abstractmethod
-import warnings
-
-import numpy as np
-from scipy import linalg
-from scipy import sparse
-from scipy.sparse import linalg as sp_linalg
-
-from .base import LinearClassifierMixin, LinearModel, _rescale_data
-from .sag import sag_solver
-from ..base import RegressorMixin
-from ..utils.extmath import safe_sparse_dot
-from ..utils.extmath import row_norms
-from ..utils import check_X_y
-from ..utils import check_array
-from ..utils import check_consistent_length
-from ..utils import compute_sample_weight
-from ..utils import column_or_1d
-from ..preprocessing import LabelBinarizer
-from ..model_selection import GridSearchCV
-from ..externals import six
-from ..metrics.scorer import check_scoring
-
-
-def _solve_sparse_cg(X, y, alpha, max_iter=None, tol=1e-3, verbose=0):
-    n_samples, n_features = X.shape
-    X1 = sp_linalg.aslinearoperator(X)
-    coefs = np.empty((y.shape[1], n_features), dtype=X.dtype)
-
-    if n_features > n_samples:
-        def create_mv(curr_alpha):
-            def _mv(x):
-                return X1.matvec(X1.rmatvec(x)) + curr_alpha * x
-            return _mv
-    else:
-        def create_mv(curr_alpha):
-            def _mv(x):
-                return X1.rmatvec(X1.matvec(x)) + curr_alpha * x
-            return _mv
-
-    for i in range(y.shape[1]):
-        y_column = y[:, i]
-
-        mv = create_mv(alpha[i])
-        if n_features > n_samples:
-            # kernel ridge
-            # w = X.T * inv(X X^t + alpha*Id) y
-            C = sp_linalg.LinearOperator(
-                (n_samples, n_samples), matvec=mv, dtype=X.dtype)
-            coef, info = sp_linalg.cg(C, y_column, tol=tol)
-            coefs[i] = X1.rmatvec(coef)
-        else:
-            # linear ridge
-            # w = inv(X^t X + alpha*Id) * X.T y
-            y_column = X1.rmatvec(y_column)
-            C = sp_linalg.LinearOperator(
-                (n_features, n_features), matvec=mv, dtype=X.dtype)
-            coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter,
-                                          tol=tol)
-        if info < 0:
-            raise ValueError("Failed with error code %d" % info)
-
-        if max_iter is None and info > 0 and verbose:
-            warnings.warn("sparse_cg did not converge after %d iterations." %
-                          info)
-
-    return coefs
-
-
-def _solve_lsqr(X, y, alpha, max_iter=None, tol=1e-3):
-    n_samples, n_features = X.shape
-    coefs = np.empty((y.shape[1], n_features), dtype=X.dtype)
-    n_iter = np.empty(y.shape[1], dtype=np.int32)
-
-    # According to the lsqr documentation, alpha = damp^2.
-    sqrt_alpha = np.sqrt(alpha)
-
-    for i in range(y.shape[1]):
-        y_column = y[:, i]
-        info = sp_linalg.lsqr(X, y_column, damp=sqrt_alpha[i],
-                              atol=tol, btol=tol, iter_lim=max_iter)
-        coefs[i] = info[0]
-        n_iter[i] = info[2]
-
-    return coefs, n_iter
-
 
 def _solve_cholesky(X, y, alpha):
     # w = inv(X^t X + alpha*Id) * X.T y
@@ -1275,7 +1184,9 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
         If you wish to standardize, please use
         :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
         on an estimator with ``normalize=False``.
-
+    store_cv_values : boolean, optional, default=False
+        If True, cross-validation values for each alpha are stored in the
+        `cv_values_` attribute.
     scoring : string, callable or None, optional, default: None
         A string (see model evaluation documentation) or
         a scorer callable object / function with signature
@@ -1333,11 +1244,12 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
     advantage of the multi-variate response support in Ridge.
     """
     def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,
-                 normalize=False, scoring=None, cv=None, class_weight=None):
+                 normalize=False, scoring=None, cv=None, class_weight=None, store_cv_values=False):
         super(RidgeClassifierCV, self).__init__(
             alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,
-            scoring=scoring, cv=cv)
+            scoring=scoring, cv=cv, store_cv_values=store_cv_values)
         self.class_weight = class_weight
+        self.store_cv_values = store_cv_values
 
     def fit(self, X, y, sample_weight=None):
         """Fit the ridge classifier.
@@ -1374,6 +1286,8 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
                              compute_sample_weight(self.class_weight, y))
 
         _BaseRidgeCV.fit(self, X, Y, sample_weight=sample_weight)
+        if self.store_cv_values:
+            self.cv_values_ = self.cv_values_
         return self
 
     @property
